
# âœ¨ GEPO: Group Expectation Policy Optimization

> **Paper**: [GEPO: Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning](https://arxiv.org/abs/2508.17850)  
> **Codebase**: Built on [`trl`](https://github.com/huggingface/trl) & [`open-r1`](https://github.com/huggingface/open-r1)

![GEPO Architecture](./MainFig.png)

> **Figure 1**: GEPO improves upon GRPO and GSPO by employing **group-level importance weights** to enhance training stability. It demonstrates superior performance in both **zero-delay (online)** and **high-delay (up to 1800s)** heterogeneous RL scenarios.

---

## ğŸ“° Latest Update: âˆ†L Normalization Integrated into HeteroRL!

<details>
<summary>ğŸ“¢ <strong> Update: Added Implementation of âˆ†L Normalization â€” Unbiased & Minimum-Variance!</strong></summary>

<br>

<h2 align="center">âœ¨ âˆ†L Normalization: Rethink Loss Aggregation in RLVR</h2>

ğŸ“… **Release Date**: September 9, 2025 (arXiv)  
ğŸ“„ **Paper**: [âˆ†L Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)  
ğŸ§‘â€ğŸ’» **Authors**: Zhiyuan He, Xufang Luo (Microsoft Research), Yike Zhang (Tsinghua), et al.  
ğŸ”— **Implementation**: Based on [Delta-L-Normalization](https://github.com/zerolllin/Delta-L-Normalization)

---

### âš¡ Why It Matters
In RLVR, response lengths vary dramatically â€” leading to **high gradient variance** and **biased updates** in existing methods (GRPO, DAPO, Dr. GRPO).  
âˆ†L Normalization solves both:
âœ… **Unbiased estimator** of true policy gradient  
âœ… **Theoretically minimal variance** (when `Î±=1`)  
âœ… **Plug-and-play** â€” <10 lines to integrate

> ğŸ’¡ **Pro Tip**:  
> - Use `Î±=1` for **minimum variance** (default, stable training).  
> - Use `Î±=0.75` for **Math tasks** â€” better utilization of long, informative responses.

</details>



<details>
<summary>ğŸ“¢ <strong>Update: Added Implementation of GSPO â€” Stable, Efficient & MoE-Friendly!</strong></summary>

<br>

<h2 align="center">âœ¨ GSPO: Group Sequence Policy Optimization for Scalable RL</h2>

ğŸ“… **Release Date**: July 28, 2025 (arXiv v2)  
ğŸ“„ **Paper**: [**Group Sequence Policy Optimization**](https://arxiv.org/abs/2507.18071)  
ğŸ§‘â€ğŸ’» **Authors**: Chujie Zheng, Shixuan Liu, Mingze Li, Bowen Yu, et al. (Qwen Team, Alibaba)  

---

### âš¡ Why It Matters
Existing methods like **GRPO** suffer from **catastrophic instability** when scaling to large models â€” especially **MoE architectures** â€” due to noisy token-level importance ratios.  
**GSPO fixes this at the root**:
âœ… **Sequence-level importance weights** â€” Matches reward granularity & reduces variance  
âœ… **Stable MoE training** â€” No â€œRouting Replayâ€ hacks needed ğŸš«  
âœ… **Higher training efficiency** â€” Achieves better performance with same compute  
âœ… **Simpler infrastructure** â€” Compatible with inference-engine likelihoods (no recompute needed)

> ğŸ’¡ **Pro Tip**:  
> - Use `clip_range=(3e-4, 4e-4)` for optimal stability (default in Qwen3 RL training).  
> - For multi-turn RL, try **GSPO-token** variant â€” enables per-token advantage while preserving sequence-level stability.

</details>


<details>
<summary>ğŸ“¢ <strong>Update: Added Implementation of Dr. GRPO â€” Unbiased & Token-Efficient!</strong></summary>

<br>

<h2 align="center">âœ¨ Dr. GRPO: Group Relative Policy Optimization Done Right</h2>

ğŸ“… **Release Date**: March 26, 2025 (arXiv)  
ğŸ“„ **Paper**: [**Understanding R1-Zero-Like Training: A Critical Perspective**](https://arxiv.org/abs/2503.20783)  
ğŸ§‘â€ğŸ’» **Authors**: Zichen Liu, Changyu Chen, Wenjun Li, et al. (Sea AI Lab, NUS, SMU)

---

### âš¡ Why It Matters
Original **GRPO** introduces **length bias** and **difficulty bias** â€” artificially inflating response lengths (especially for *incorrect* answers) and skewing updates toward â€œeasierâ€ questions.  
**Dr. GRPO removes these biases at the source**:
âœ… **Unbiased gradient estimator** â€” Faithfully implements true policy gradient objective  
âœ… **Token-efficient training** â€” Prevents wasteful generation of long, incorrect responses ğŸš«ğŸ“  
âœ… **Plug-and-play replacement** â€” Drop-in substitute for GRPO with minimal code change  
âœ… **Preserves reasoning performance** â€” Matches or exceeds GRPOâ€™s final accuracy with less compute

> ğŸ’¡ **Pro Tip**:  
> - Use Dr. GRPO when you want **stable length growth** (only for correct reasoning, not noise).  
> - Combine with **âˆ†L Normalization** for double variance reduction + unbiasedness.

</details>

<details>
<summary>ğŸ“¢ <strong>Update: Added Implementation of BNPO â€” Adaptive, Low-Variance & Generalizes GRPO!</strong></summary>

<br>

<h2 align="center">âœ¨ BNPO: Beta Normalization Policy Optimization for Stable RL Training</h2>

ğŸ“… **Release Date**: June 3, 2025 (arXiv)  
ğŸ“„ **Paper**: [**BNPO: Beta Normalization Policy Optimization**](https://arxiv.org/abs/2506.02864)  
ğŸ§‘â€ğŸ’» **Authors**: Changyi Xiao, Mengdi Zhang, Yixin Cao (Fudan University, Meituan)  

---

### âš¡ Why It Matters
Current RL methods like **GRPO** and **REINFORCE** use **static reward normalization** â€” fixed throughout training â€” which fails to adapt to the evolving policy distribution, leading to unstable gradients and suboptimal convergence.  
**BNPO solves this with dynamic, theoretically grounded normalization**:
âœ… **Adaptive Beta normalization** â€” Parameters `(Î±, Î²)` update dynamically with policy evolution  
âœ… **Proven variance reduction** â€” Theoretically minimizes gradient variance under binary rewards  
âœ… **Generalizes GRPO & REINFORCE** â€” Reduces to them under specific `(Î±, Î²)` settings  
âœ… **Handles complex rewards** â€” Via novel *Advantage Decomposition* mechanism

> ğŸ’¡ **Pro Tip**:  
> - BNPO automatically sets `Î± = (1+a)/3`, `Î² = (1+b)/3` â€” no manual tuning needed.  
> - Use Advantage Decomposition when combining multiple reward signals (e.g., accuracy + format).

</details>



---



## ğŸ§  Importance Weight Computation (Policy Optimization Methods)

```python
# Token-level importance ratio (e.g., GRPO, Dr. GRPO, BNPO)
if self.loss_type in ["grpo", "dr_grpo", "bnpo"]:
    coef_1 = learner_token_p / sampler_token_p

# Sequence-level importance ratio (GSPO)
elif self.loss_type == "gspo":
    coef_1 = learner_seq_p / sampler_seq_p

# Group-level importance ratio (GEPO â€” Ours)
elif self.loss_type == "gepo":
    normalized_q = sampler_seq_p.detach() / sampler_seq_p.sum().detach()
    coef_1 = learner_seq_p / (normalized_q * sampler_seq_p).sum()
```

> ğŸ“Œ **Note**: GEPO computes importance weights at the **group level**, stabilizing training under heterogeneous sampling delays.

---

## âš™ï¸ Heterogeneous Reinforcement Learning Setup

> ğŸ“ Enter the project directory first. Adjust paths in scripts if your directory differs.

### 1ï¸âƒ£ Launch the Learner (4Ã—A100 80GB)

```bash
cd ./open-r1
CUDA_VISIBLE_DEVICES=0,1,2,3 \
bash sh_dir/HeteroRL_Learner_4gpus.sh \
    learner_script_checkpoint \
    GEPO_think_1th \
    1 \
    v6b \
    gepo \
    1L2S_GEPO_diff32_think
```

### 2ï¸âƒ£ Launch Samplers (Run in Sequence)

> ğŸ”„ To resume from checkpoint: Set `model_name_or_path` to your checkpoint path.

```bash
# Launch 4 sampler processes in background
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 0 &
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 1 &
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 2 &
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 3 &
```

---

## ğŸŒ Online Reinforcement Learning (4Ã—A100 80GB)

Supports multiple loss types:
- [`grpo`](https://arxiv.org/abs/2402.03300)
- [`bnpo`](https://arxiv.org/abs/2506.02864)
- [`dr_grpo`](https://arxiv.org/abs/2503.20783)
- [`gspo`](https://arxiv.org/abs/2507.18071)
- [`âˆ†L Norm`](https://arxiv.org/abs/2509.07558)
- **`gepo` (ours)** ğŸ‘ˆ

```bash
cd /userhome/Research_HUB/GPG/open-r1
CUDA_VISIBLE_DEVICES="0,1,2,3" \
MASTER_PORT=29510 \
bash sh_dir/Online_gXpo_4gpus.sh gepo
```

---

## ğŸ“š Citation

If you use **GEPO** or find this code helpful, please cite:

```bibtex
@misc{gepo2025,
  title     = {Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning},
  author    = {Han Zhang and Ruibin Zheng and Zexuan Yi and Zhuo Zhang and Hanyang Peng and Hui Wang and Zike Yuan and Cai Ke and Shiwei Chen and Jiacheng Yang and Yangning Li and Xiang Li and Jiangyue Yan and Yaoqi Liu and Liwen Jing and Jiayin Qi and Ruifeng Xu and Binxing Fang and Yue Yu},
  year      = {2025},
  eprint    = {2508.17850},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url       = {https://arxiv.org/abs/2508.17850}
}
```

