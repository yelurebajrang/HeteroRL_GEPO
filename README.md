
# âœ¨ HeteroRL: Heterogeneous Reinforcement Learning

> **Paper**: [GEPO: Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning](https://arxiv.org/abs/2508.17850)  
> **Codebase**: Built on [`trl`](https://github.com/huggingface/trl) & [`open-r1`](https://github.com/huggingface/open-r1)


<details open>
<summary>ğŸ“¢ <strong> BREAKING: GEPO â€” The Algorithm That Makes Decentralized AI Training Possible!</strong></summary>

<br>

<h2 align="center">âœ¨ GEPO: Group Expectation Policy Optimization for Heterogeneous RL</h2>

ğŸ“… **Release Date**: Aug 25, 2025 (arXiv)  
ğŸ“„ **Paper**: [Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning](https://arxiv.org/abs/2508.17850)  
ğŸ§‘â€ğŸ’» **Authors**: Han Zhang, Ruibin Zheng, et al. (Pengcheng Lab / Heterogeneous Large Model Research Team)  

---

### âš¡ Why It Matters

Training giant AI models now requires global, decentralized compute. But network delays cause â€œpolicy staleness,â€ making traditional RL algorithms (like GRPO, GSPO) **crash** due to exploding gradient variance.

**GEPO solves this.** By replacing unstable per-token weights with **Group Expectation Importance Weighting**, it exponentially reduces variance under high latency â€” enabling stable training even with **1800-second delays**.

âœ… **Theoretically Proven**: Exponentially reduces importance sampling variance (Theorem 1).  
âœ… **Extremely Robust**: Only **3% performance drop** under extreme 1800s latency.  
âœ… **Plug-and-Play**: Easy to integrate â€” modifies only the importance weight calculation.  
âœ… **Better Everywhere**: Outperforms GRPO/GSPO even in zero-delay (online) settings.

> ğŸ“Š **Key Results (Qwen3-1.7B)**:
> - **Zero-Delay**: GEPO Last = **41.4** vs. GSPO Last = **24.3** (+17.1 gain).  
> - **High-Delay (64 steps)**: GEPO Last = **43.5** (no drop) vs. GSPO Last = **20.9**.  
> - **Extreme Test (1800s)**: Performance degradation **< 3%**.

---
### ğŸ“‹ Full Benchmark Comparison

| Method          | AMC2023     | AIME2024    | AIME2025    | MATH500     | Average     |
|-----------------|-------------|-------------|-------------|-------------|-------------|
|                 | Best / Last | Best / Last | Best / Last | Best / Last | Best / Last |
| **Qwen3-1.7B**  | 25.6 / â€”    | 1.6 / â€”     | 3.9 / â€”     | 54.7 / â€”    | 21.5 / â€”    |
|                 |             |             |             |             |             |
| **Max Delay = 0 (Online RL)** |             |             |             |             |             |
| BNPO            | 54.3 / 0.0  | 18.4 / 0.0  | 19.1 / 0.0  | 78.7 / 0.0  | 42.6 / 0.0  |
| Dr.GRPO         | 53.4 / 14.3 | 19.1 / 1.6  | 18.8 / 2.0  | 78.6 / 35.9 | 42.5 / 13.5 |
| GRPO            | 56.3 / 23.4 | 20.7 / 0.4  | 19.9 / 2.3  | 79.8 / 49.7 | 44.2 / 19.0 |
| GSPO            | 54.1 / 27.8 | **23.8** / 3.1 | **20.7** / 4.3 | 79.9 / 62.1 | 44.6 / 24.3 |
| **GEPO (Ours)** | **56.9** / **56.9** | 21.9 / **16.4** | 20.3 / **14.1** | **80.4** / **78.1** | **44.9** / **41.4** |
|                 |             |             |             |             |             |
| **Max Delay = 64 (Hetero RL)** |             |             |             |             |             |
| BNPO            | 45.0 / 43.1 | 12.1 / 11.3 | 12.5 / 10.1 | 71.1 / 69.3 | 35.2 / 33.5 |
| Dr.GRPO         | 48.4 / 48.4 | 17.2 / 17.2 | 14.8 / 14.8 | 73.9 / 73.9 | 38.6 / 38.6 |
| GRPO            | 46.6 / 46.6 | 19.1 / 14.5 | 14.8 / 14.8 | 74.9 / 74.9 | 38.9 / 37.7 |
| GSPO            | **54.4** / 23.8 | 17.6 / 1.6  | 17.6 / 2.7  | 78.2 / 55.6 | 41.9 / 20.9 |
| **GEPO (Ours)** | 53.8 / **53.8** | **21.9** / **21.9** | **18.8** / **18.8** | **79.6** / **79.6** | **43.5** / **43.5** |

---


### ğŸ§  The Core Idea: Think Groups, Not Tokens

Traditional methods use `p(y|x) / q(y|x)`, which explodes when `q(y|x)` is small. GEPOâ€™s genius is simple:

**Group Expectation Weight:**
`w_GEPO(y|x) = p(y|x) / ÃŠ_q[q(y|x)]`

Where `ÃŠ_q[q(y|x)]` is estimated from a group of responses `{y1...yG}` for the same prompt `x`:
`ÃŠ_q[q(y|x)] â‰ˆ Î£(q(yi|x)Â²) / Î£(q(yi|x))`

This group-level denominator **smooths out wild fluctuations**, preventing gradient explosions and keeping training stable â€” no matter how stale the data is.

![GEPO Architecture](./MainFig.png)

> **Figure 1**: GEPO improves upon GRPO and GSPO by employing **group-level importance weights** to enhance training stability. It demonstrates superior performance in both **zero-delay (online)** and **high-delay (up to 1800s)** heterogeneous RL scenarios.

---

### ğŸš€ The Future: Decentralized AI is Here

GEPO is the engine of **HeteroRL**, a framework that decouples sampling and learning across global nodes. This isnâ€™t just an algorithm â€” itâ€™s the foundation for community-driven, globally distributed AI training.

> ğŸ’¡ **Pro Tip**:  
> - Use GEPO as your **default RL algorithm** â€” itâ€™s more stable everywhere.  
> - For maximum robustness in production, combine it with the â€œDefensive Samplingâ€ mechanism (Appendix F).

</details>

---

## ğŸ“° Latest Update: âˆ†L Normalization and GMPO Integrated into HeteroRL!

<details>
<summary>ğŸ“¢ <strong>Sep 26 Update(ğŸ–±ï¸): Added Implementation of GMPO â€” Geometric-Mean Policy Optimization!</strong></summary>

<br>

<h2 align="center">âœ¨ Geometric-Mean Policy Optimization (GMPO): Stabilizing GRPO with Outlier-Robust Aggregation</h2>

ğŸ“… **Release Date**: Jul 28, 2025 (arXiv)  
ğŸ“„ **Paper**: [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)  
ğŸ§‘â€ğŸ’» **Authors**: Yuzhong Zhao, Yue Liu (UCAS), Junpeng Liu (CUHK), Jingye Chen (HKUST), and Microsoft Research Team  
ğŸ”— **Implementation**: Based on [GMPO](https://github.com/callsys/GMPO)

---

### âš¡ Why It Matters
GRPO optimizes the **arithmetic mean** of token-level rewards, which is highly sensitive to **outlier importance-weighted rewards**, causing unstable policy updates and extreme importance sampling ratios.  
GMPO addresses this by switching to the **geometric mean**, which is inherently **robust to outliers** and leads to:
âœ… **Stable importance sampling ratios** (narrower range, fewer extremes)  
âœ… **Lower training variance** and **more reliable gradients**  
âœ… **Enhanced exploration** via wider clipping (e.g., `(eâ»â°Â·â´, eâ°Â·â´)`) without sacrificing stability  
âœ… **Consistent gains**: **+4.1%** on math benchmarks and **+1.4%** on multimodal reasoning (Geometry3K)

> ğŸ’¡ **Pro Tips**:  
> - Use **token-level clipping** (not sequence-level) for finer gradient control.  
> - Set clipping range to `(eâ»â°Â·â´, eâ°Â·â´)` to balance exploration and stability.  
> - GMPO maintains **higher token entropy** and **lower KL divergence** from the pre-RL model â€” ideal for scalable RL training.

</details>

<details>
<summary>ğŸ“¢ <strong> Sep 17 Update(ğŸ–±ï¸): Added Implementation of âˆ†L Normalization â€” Unbiased & Minimum-Variance!</strong></summary>

<br>

<h2 align="center">âœ¨ âˆ†L Normalization: Rethink Loss Aggregation in RLVR</h2>

ğŸ“… **Release Date**: Sep 9, 2025 (arXiv)  
ğŸ“„ **Paper**: [âˆ†L Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)  
ğŸ§‘â€ğŸ’» **Authors**: Zhiyuan He, Xufang Luo (Microsoft Research), Yike Zhang (Tsinghua), et al.  
ğŸ”— **Implementation**: Based on [Delta-L-Normalization](https://github.com/zerolllin/Delta-L-Normalization)

---

### âš¡ Why It Matters
In RLVR, response lengths vary dramatically â€” leading to **high gradient variance** and **biased updates** in existing methods (GRPO, DAPO, Dr. GRPO).  
âˆ†L Normalization solves both:
âœ… **Unbiased estimator** of true policy gradient  
âœ… **Theoretically minimal variance** (when `Î±=1`)  
âœ… **Plug-and-play** â€” <10 lines to integrate

> ğŸ’¡ **Pro Tip**:  
> - Use `Î±=1` for **minimum variance** (default, stable training).  
> - Use `Î±=0.75` for **Math tasks** â€” better utilization of long, informative responses.

</details>



<details>
<summary>ğŸ“¢ <strong>Update: Added Implementation of GSPO â€” Stable, Efficient & MoE-Friendly!</strong></summary>

<br>

<h2 align="center">âœ¨ GSPO: Group Sequence Policy Optimization for Scalable RL</h2>

ğŸ“… **Release Date**: July 28, 2025 (arXiv v2)  
ğŸ“„ **Paper**: [**Group Sequence Policy Optimization**](https://arxiv.org/abs/2507.18071)  
ğŸ§‘â€ğŸ’» **Authors**: Chujie Zheng, Shixuan Liu, Mingze Li, Bowen Yu, et al. (Qwen Team, Alibaba)  

---

### âš¡ Why It Matters
Existing methods like **GRPO** suffer from **catastrophic instability** when scaling to large models â€” especially **MoE architectures** â€” due to noisy token-level importance ratios.  
**GSPO fixes this at the root**:
âœ… **Sequence-level importance weights** â€” Matches reward granularity & reduces variance  
âœ… **Stable MoE training** â€” No â€œRouting Replayâ€ hacks needed ğŸš«  
âœ… **Higher training efficiency** â€” Achieves better performance with same compute  
âœ… **Simpler infrastructure** â€” Compatible with inference-engine likelihoods (no recompute needed)

> ğŸ’¡ **Pro Tip**:  
> - Use `clip_range=(3e-4, 4e-4)` for optimal stability (default in Qwen3 RL training).  
> - For multi-turn RL, try **GSPO-token** variant â€” enables per-token advantage while preserving sequence-level stability.

</details>


<details>
<summary>ğŸ“¢ <strong>Update: Added Implementation of Dr. GRPO â€” Unbiased & Token-Efficient!</strong></summary>

<br>

<h2 align="center">âœ¨ Dr. GRPO: Group Relative Policy Optimization Done Right</h2>

ğŸ“… **Release Date**: March 26, 2025 (arXiv)  
ğŸ“„ **Paper**: [**Understanding R1-Zero-Like Training: A Critical Perspective**](https://arxiv.org/abs/2503.20783)  
ğŸ§‘â€ğŸ’» **Authors**: Zichen Liu, Changyu Chen, Wenjun Li, et al. (Sea AI Lab, NUS, SMU)

---

### âš¡ Why It Matters
Original **GRPO** introduces **length bias** and **difficulty bias** â€” artificially inflating response lengths (especially for *incorrect* answers) and skewing updates toward â€œeasierâ€ questions.  
**Dr. GRPO removes these biases at the source**:
âœ… **Unbiased gradient estimator** â€” Faithfully implements true policy gradient objective  
âœ… **Token-efficient training** â€” Prevents wasteful generation of long, incorrect responses ğŸš«ğŸ“  
âœ… **Plug-and-play replacement** â€” Drop-in substitute for GRPO with minimal code change  
âœ… **Preserves reasoning performance** â€” Matches or exceeds GRPOâ€™s final accuracy with less compute

> ğŸ’¡ **Pro Tip**:  
> - Use Dr. GRPO when you want **stable length growth** (only for correct reasoning, not noise).  
> - Combine with **âˆ†L Normalization** for double variance reduction + unbiasedness.

</details>

<details>
<summary>ğŸ“¢ <strong>Update: Added Implementation of BNPO â€” Adaptive, Low-Variance & Generalizes GRPO!</strong></summary>

<br>

<h2 align="center">âœ¨ BNPO: Beta Normalization Policy Optimization for Stable RL Training</h2>

ğŸ“… **Release Date**: June 3, 2025 (arXiv)  
ğŸ“„ **Paper**: [**BNPO: Beta Normalization Policy Optimization**](https://arxiv.org/abs/2506.02864)  
ğŸ§‘â€ğŸ’» **Authors**: Changyi Xiao, Mengdi Zhang, Yixin Cao (Fudan University, Meituan)  

---

### âš¡ Why It Matters
Current RL methods like **GRPO** and **REINFORCE** use **static reward normalization** â€” fixed throughout training â€” which fails to adapt to the evolving policy distribution, leading to unstable gradients and suboptimal convergence.  
**BNPO solves this with dynamic, theoretically grounded normalization**:
âœ… **Adaptive Beta normalization** â€” Parameters `(Î±, Î²)` update dynamically with policy evolution  
âœ… **Proven variance reduction** â€” Theoretically minimizes gradient variance under binary rewards  
âœ… **Generalizes GRPO & REINFORCE** â€” Reduces to them under specific `(Î±, Î²)` settings  
âœ… **Handles complex rewards** â€” Via novel *Advantage Decomposition* mechanism

> ğŸ’¡ **Pro Tip**:  
> - BNPO automatically sets `Î± = (1+a)/3`, `Î² = (1+b)/3` â€” no manual tuning needed.  
> - Use Advantage Decomposition when combining multiple reward signals (e.g., accuracy + format).

</details>



---



## ğŸ§  Importance Weight Computation (Policy Optimization Methods)

```python
# Token-level importance ratio (e.g., GRPO, Dr. GRPO, BNPO)
if self.loss_type in ["grpo", "dr_grpo", "bnpo"]:
    coef_1 = learner_token_p / sampler_token_p

# Sequence-level importance ratio (GSPO)
elif self.loss_type == "gspo":
    coef_1 = learner_seq_p / sampler_seq_p

# Group-level importance ratio (GEPO â€” Ours)
elif self.loss_type == "gepo":
    normalized_q = sampler_seq_p.detach() / sampler_seq_p.sum().detach()
    coef_1 = learner_seq_p / (normalized_q * sampler_seq_p).sum()
```

> ğŸ“Œ **Note**: GEPO computes importance weights at the **group level**, stabilizing training under heterogeneous sampling delays.

---

## âš™ï¸ Heterogeneous Reinforcement Learning Setup

> ğŸ“ Enter the project directory first. Adjust paths in scripts if your directory differs.

### 1ï¸âƒ£ Launch the Learner (4Ã—A100 80GB)

```bash
cd ./open-r1
CUDA_VISIBLE_DEVICES=0,1,2,3 bash sh_dir/HeteroRL_Learner_4gpus.sh learner_script_checkpoint GEPO_think_1th 1 v6b gepo 1L2S_GEPO_diff32_think
```

### 2ï¸âƒ£ Launch Samplers (Run in Sequence)

> ğŸ”„ To resume from checkpoint: Set `model_name_or_path` to your checkpoint path.

```bash
# Launch 4 sampler processes in background
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 0 &
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 1 &
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 2 &
bash sh_dir/HeteroRL_Sampler_4gpus.sh sampler_script_checkpoint GEPO_think_1th v6b gepo 1L2S_GEPO_diff32_think 3 &
```

---

## ğŸŒ Online Reinforcement Learning (4Ã—A100 80GB)

Supports multiple policy optimization methods:
- [`GRPO`](https://arxiv.org/abs/2402.03300): DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
- [`BNPO`](https://arxiv.org/abs/2506.02864): Beta Normalization Policy Optimization
- [`Dr.GRPO`](https://arxiv.org/abs/2503.20783): Understanding R1-Zero-Like Training: A Critical Perspective
- [`GSPO`](https://arxiv.org/abs/2507.18071): Group Sequence Policy Optimization
- [`âˆ†L Normalization`](https://arxiv.org/abs/2509.07558): Rethink Loss Aggregation in RLVR
- **`gepo` (ours)** ğŸ‘ˆ

```bash
cd ./open-r1
CUDA_VISIBLE_DEVICES="0,1,2,3" MASTER_PORT=29510 bash sh_dir/Online_gXpo_4gpus.sh gepo
```

---

## ğŸ“š Citation

If you use **GEPO** or find this code helpful, please cite:

```bibtex
@misc{gepo2025,
  title     = {Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning},
  author    = {Han Zhang and Ruibin Zheng and Zexuan Yi and Zhuo Zhang and Hanyang Peng and Hui Wang and Zike Yuan and Cai Ke and Shiwei Chen and Jiacheng Yang and Yangning Li and Xiang Li and Jiangyue Yan and Yaoqi Liu and Liwen Jing and Jiayin Qi and Ruifeng Xu and Binxing Fang and Yue Yu},
  year      = {2025},
  eprint    = {2508.17850},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url       = {https://arxiv.org/abs/2508.17850}
}
```

